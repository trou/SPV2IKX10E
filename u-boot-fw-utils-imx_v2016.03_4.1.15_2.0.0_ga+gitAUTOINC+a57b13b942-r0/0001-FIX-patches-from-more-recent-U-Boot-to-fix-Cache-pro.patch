From 31346c2dca804277e3e6cf9787cacca1d77ebb26 Mon Sep 17 00:00:00 2001
From: "Proske, Matthias (BSH)" <Matthias.Proske@bshg.com>
Date: Fri, 15 Jun 2018 19:31:58 +0200
Subject: [PATCH] FIX: patches from more recent U-Boot to fix Cache problem

These patches were taken from the current master (2018-06-15)  606fddd

It fixes the cache problem, that according to
https://bugzilla.redhat.com/show_bug.cgi?id=1318788 comes with GCC 6.2

First tests were succesfull (were the non-fixed version failed).

The problems that occured that the CPU would halt or show a data abort
exception when turning off the data cache. This could be provoked by
using the U-Boot command dcache off.
Unfortunately this error only occurs in certain situations (certain)
environments, and reading a environment variable before executing a
command can already change the behaviour.
Effectively the error occured when booting a system, as deactivation of
TLB is needed, and for this the data cache needed to be deactivated.
Depending on the current system state (number of systems installed,
environment) we would see the error.
---
 arch/arm/cpu/armv7/Makefile       |   2 +-
 arch/arm/cpu/armv7/cache_v7.c     | 196 +++++---------------------------------
 arch/arm/cpu/armv7/cache_v7_asm.S | 152 +++++++++++++++++++++++++++++
 arch/arm/include/asm/armv7.h      |  45 +++++----
 arch/arm/include/asm/barriers.h   |  50 ++++++++++
 arch/arm/include/asm/cache.h      |   2 +
 arch/arm/lib/cache.c              |  25 +++++
 7 files changed, 277 insertions(+), 195 deletions(-)
 create mode 100644 arch/arm/cpu/armv7/cache_v7_asm.S
 create mode 100644 arch/arm/include/asm/barriers.h

diff --git a/arch/arm/cpu/armv7/Makefile b/arch/arm/cpu/armv7/Makefile
index 45f346c..328c4b1 100644
--- a/arch/arm/cpu/armv7/Makefile
+++ b/arch/arm/cpu/armv7/Makefile
@@ -7,7 +7,7 @@
 
 extra-y	:= start.o
 
-obj-y	+= cache_v7.o
+obj-y	+= cache_v7.o cache_v7_asm.o
 
 obj-y	+= cpu.o cp15.o
 obj-y	+= syslib.o
diff --git a/arch/arm/cpu/armv7/cache_v7.c b/arch/arm/cpu/armv7/cache_v7.c
index 94ff488..99484c2 100644
--- a/arch/arm/cpu/armv7/cache_v7.c
+++ b/arch/arm/cpu/armv7/cache_v7.c
@@ -1,49 +1,22 @@
+// SPDX-License-Identifier: GPL-2.0+
 /*
  * (C) Copyright 2010
  * Texas Instruments, <www.ti.com>
  * Aneesh V <aneesh@ti.com>
- *
- * SPDX-License-Identifier:	GPL-2.0+
  */
 #include <linux/types.h>
 #include <common.h>
 #include <asm/armv7.h>
 #include <asm/utils.h>
 
-#define ARMV7_DCACHE_INVAL_ALL		1
-#define ARMV7_DCACHE_CLEAN_INVAL_ALL	2
-#define ARMV7_DCACHE_INVAL_RANGE	3
-#define ARMV7_DCACHE_CLEAN_INVAL_RANGE	4
+#define ARMV7_DCACHE_INVAL_RANGE	1
+#define ARMV7_DCACHE_CLEAN_INVAL_RANGE	2
 
 #ifndef CONFIG_SYS_DCACHE_OFF
-static int check_cache_range(unsigned long start, unsigned long stop)
-{
-	int ok = 1;
-
-	if (start & (CONFIG_SYS_CACHELINE_SIZE - 1))
-		ok = 0;
-
-	if (stop & (CONFIG_SYS_CACHELINE_SIZE - 1))
-		ok = 0;
-
-	if (!ok)
-		debug("CACHE: Misaligned operation at range [%08lx, %08lx]\n",
-			start, stop);
-
-	return ok;
-}
-
-/*
- * Write the level and type you want to Cache Size Selection Register(CSSELR)
- * to get size details from Current Cache Size ID Register(CCSIDR)
- */
-static void set_csselr(u32 level, u32 type)
-{
-	u32 csselr = level << 1 | type;
 
-	/* Write to Cache Size Selection Register(CSSELR) */
-	asm volatile ("mcr p15, 2, %0, c0, c0, 0" : : "r" (csselr));
-}
+/* Asm functions from cache_v7_asm.S */
+void v7_flush_dcache_all(void);
+void v7_invalidate_dcache_all(void);
 
 static u32 get_ccsidr(void)
 {
@@ -54,118 +27,6 @@ static u32 get_ccsidr(void)
 	return ccsidr;
 }
 
-static u32 get_clidr(void)
-{
-	u32 clidr;
-
-	/* Read current CP15 Cache Level ID Register */
-	asm volatile ("mrc p15,1,%0,c0,c0,1" : "=r" (clidr));
-	return clidr;
-}
-
-static void v7_inval_dcache_level_setway(u32 level, u32 num_sets,
-					 u32 num_ways, u32 way_shift,
-					 u32 log2_line_len)
-{
-	int way, set;
-	u32 setway;
-
-	/*
-	 * For optimal assembly code:
-	 *	a. count down
-	 *	b. have bigger loop inside
-	 */
-	for (way = num_ways - 1; way >= 0 ; way--) {
-		for (set = num_sets - 1; set >= 0; set--) {
-			setway = (level << 1) | (set << log2_line_len) |
-				 (way << way_shift);
-			/* Invalidate data/unified cache line by set/way */
-			asm volatile ("	mcr p15, 0, %0, c7, c6, 2"
-					: : "r" (setway));
-		}
-	}
-	/* DSB to make sure the operation is complete */
-	DSB;
-}
-
-static void v7_clean_inval_dcache_level_setway(u32 level, u32 num_sets,
-					       u32 num_ways, u32 way_shift,
-					       u32 log2_line_len)
-{
-	int way, set;
-	u32 setway;
-
-	/*
-	 * For optimal assembly code:
-	 *	a. count down
-	 *	b. have bigger loop inside
-	 */
-	for (way = num_ways - 1; way >= 0 ; way--) {
-		for (set = num_sets - 1; set >= 0; set--) {
-			setway = (level << 1) | (set << log2_line_len) |
-				 (way << way_shift);
-			/*
-			 * Clean & Invalidate data/unified
-			 * cache line by set/way
-			 */
-			asm volatile ("	mcr p15, 0, %0, c7, c14, 2"
-					: : "r" (setway));
-		}
-	}
-	/* DSB to make sure the operation is complete */
-	DSB;
-}
-
-static void v7_maint_dcache_level_setway(u32 level, u32 operation)
-{
-	u32 ccsidr;
-	u32 num_sets, num_ways, log2_line_len, log2_num_ways;
-	u32 way_shift;
-
-	set_csselr(level, ARMV7_CSSELR_IND_DATA_UNIFIED);
-
-	ccsidr = get_ccsidr();
-
-	log2_line_len = ((ccsidr & CCSIDR_LINE_SIZE_MASK) >>
-				CCSIDR_LINE_SIZE_OFFSET) + 2;
-	/* Converting from words to bytes */
-	log2_line_len += 2;
-
-	num_ways  = ((ccsidr & CCSIDR_ASSOCIATIVITY_MASK) >>
-			CCSIDR_ASSOCIATIVITY_OFFSET) + 1;
-	num_sets  = ((ccsidr & CCSIDR_NUM_SETS_MASK) >>
-			CCSIDR_NUM_SETS_OFFSET) + 1;
-	/*
-	 * According to ARMv7 ARM number of sets and number of ways need
-	 * not be a power of 2
-	 */
-	log2_num_ways = log_2_n_round_up(num_ways);
-
-	way_shift = (32 - log2_num_ways);
-	if (operation == ARMV7_DCACHE_INVAL_ALL) {
-		v7_inval_dcache_level_setway(level, num_sets, num_ways,
-				      way_shift, log2_line_len);
-	} else if (operation == ARMV7_DCACHE_CLEAN_INVAL_ALL) {
-		v7_clean_inval_dcache_level_setway(level, num_sets, num_ways,
-						   way_shift, log2_line_len);
-	}
-}
-
-static void v7_maint_dcache_all(u32 operation)
-{
-	u32 level, cache_type, level_start_bit = 0;
-	u32 clidr = get_clidr();
-
-	for (level = 0; level < 7; level++) {
-		cache_type = (clidr >> level_start_bit) & 0x7;
-		if ((cache_type == ARMV7_CLIDR_CTYPE_DATA_ONLY) ||
-		    (cache_type == ARMV7_CLIDR_CTYPE_INSTRUCTION_DATA) ||
-		    (cache_type == ARMV7_CLIDR_CTYPE_UNIFIED))
-			v7_maint_dcache_level_setway(level, operation);
-		level_start_bit += 3;
-	}
-}
-
 static void v7_dcache_clean_inval_range(u32 start, u32 stop, u32 line_len)
 {
 	u32 mva;
@@ -182,27 +43,8 @@ static void v7_dcache_inval_range(u32 start, u32 stop, u32 line_len)
 {
 	u32 mva;
 
-	/*
-	 * If start address is not aligned to cache-line do not
-	 * invalidate the first cache-line
-	 */
-	if (start & (line_len - 1)) {
-		printf("ERROR: %s - start address is not aligned - 0x%08x\n",
-			__func__, start);
-		/* move to next cache line */
-		start = (start + line_len - 1) & ~(line_len - 1);
-	}
-
-	/*
-	 * If stop address is not aligned to cache-line do not
-	 * invalidate the last cache-line
-	 */
-	if (stop & (line_len - 1)) {
-		printf("ERROR: %s - stop address is not aligned - 0x%08x\n",
-			__func__, stop);
-		/* align to the beginning of this cache line */
-		stop &= ~(line_len - 1);
-	}
+	if (!check_cache_range(start, stop))
+		return;
 
 	for (mva = start; mva < stop; mva = mva + line_len) {
 		/* DCIMVAC - Invalidate data cache by MVA to PoC */
@@ -232,7 +74,7 @@ static void v7_dcache_maint_range(u32 start, u32 stop, u32 range_op)
 	}
 
 	/* DSB to make sure the operation is complete */
-	DSB;
+	dsb();
 }
 
 /* Invalidate TLB */
@@ -245,14 +87,14 @@ static void v7_inval_tlb(void)
 	/* Invalidate entire instruction TLB */
 	asm volatile ("mcr p15, 0, %0, c8, c5, 0" : : "r" (0));
 	/* Full system DSB - make sure that the invalidation is complete */
-	DSB;
+	dsb();
 	/* Full system ISB - make sure the instruction stream sees it */
-	ISB;
+	isb();
 }
 
 void invalidate_dcache_all(void)
 {
-	v7_maint_dcache_all(ARMV7_DCACHE_INVAL_ALL);
+	v7_invalidate_dcache_all();
 
 	v7_outer_cache_inval_all();
 }
@@ -263,7 +105,7 @@ void invalidate_dcache_all(void)
  */
 void flush_dcache_all(void)
 {
-	v7_maint_dcache_all(ARMV7_DCACHE_CLEAN_INVAL_ALL);
+	v7_flush_dcache_all();
 
 	v7_outer_cache_flush_all();
 }
@@ -316,6 +158,14 @@ void flush_dcache_all(void)
 {
 }
 
+void invalidate_dcache_range(unsigned long start, unsigned long stop)
+{
+}
+
+void flush_dcache_range(unsigned long start, unsigned long stop)
+{
+}
+
 void arm_init_before_mmu(void)
 {
 }
@@ -343,10 +193,10 @@ void invalidate_icache_all(void)
 	asm volatile ("mcr p15, 0, %0, c7, c5, 6" : : "r" (0));
 
 	/* Full system DSB - make sure that the invalidation is complete */
-	DSB;
+	dsb();
 
 	/* ISB - make sure the instruction stream sees it */
-	ISB;
+	isb();
 }
 #else
 void invalidate_icache_all(void)
diff --git a/arch/arm/cpu/armv7/cache_v7_asm.S b/arch/arm/cpu/armv7/cache_v7_asm.S
new file mode 100644
index 0000000..e38d72f
--- /dev/null
+++ b/arch/arm/cpu/armv7/cache_v7_asm.S
@@ -0,0 +1,152 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+
+#include <config.h>
+#include <linux/linkage.h>
+#include <linux/sizes.h>
+#include <asm/system.h>
+
+#if CONFIG_IS_ENABLED(SYS_THUMB_BUILD)
+#define ARM(x...)
+#define THUMB(x...)	x
+#else
+#define ARM(x...)	x
+#define THUMB(x...)
+#endif
+
+/*
+ *	v7_flush_dcache_all()
+ *
+ *	Flush the whole D-cache.
+ *
+ *	Corrupted registers: r0-r7, r9-r11 (r6 only in Thumb mode)
+ *
+ *	Note: copied from arch/arm/mm/cache-v7.S of Linux 4.4
+ */
+ENTRY(__v7_flush_dcache_all)
+	dmb					@ ensure ordering with previous memory accesses
+	mrc	p15, 1, r0, c0, c0, 1		@ read clidr
+	mov	r3, r0, lsr #23			@ move LoC into position
+	ands	r3, r3, #7 << 1			@ extract LoC*2 from clidr
+	beq	finished			@ if loc is 0, then no need to clean
+start_flush_levels:
+	mov	r10, #0				@ start clean at cache level 0
+flush_levels:
+	add	r2, r10, r10, lsr #1		@ work out 3x current cache level
+	mov	r1, r0, lsr r2			@ extract cache type bits from clidr
+	and	r1, r1, #7			@ mask of the bits for current cache only
+	cmp	r1, #2				@ see what cache we have at this level
+	blt	skip				@ skip if no cache, or just i-cache
+	mcr	p15, 2, r10, c0, c0, 0		@ select current cache level in cssr
+	isb					@ isb to sych the new cssr&csidr
+	mrc	p15, 1, r1, c0, c0, 0		@ read the new csidr
+	and	r2, r1, #7			@ extract the length of the cache lines
+	add	r2, r2, #4			@ add 4 (line length offset)
+	movw	r4, #0x3ff
+	ands	r4, r4, r1, lsr #3		@ find maximum number on the way size
+	clz	r5, r4				@ find bit position of way size increment
+	movw	r7, #0x7fff
+	ands	r7, r7, r1, lsr #13		@ extract max number of the index size
+loop1:
+	mov	r9, r7				@ create working copy of max index
+loop2:
+ ARM(	orr	r11, r10, r4, lsl r5	)	@ factor way and cache number into r11
+ THUMB(	lsl	r6, r4, r5		)
+ THUMB(	orr	r11, r10, r6		)	@ factor way and cache number into r11
+ ARM(	orr	r11, r11, r9, lsl r2	)	@ factor index number into r11
+ THUMB(	lsl	r6, r9, r2		)
+ THUMB(	orr	r11, r11, r6		)	@ factor index number into r11
+	mcr	p15, 0, r11, c7, c14, 2		@ clean & invalidate by set/way
+	subs	r9, r9, #1			@ decrement the index
+	bge	loop2
+	subs	r4, r4, #1			@ decrement the way
+	bge	loop1
+skip:
+	add	r10, r10, #2			@ increment cache number
+	cmp	r3, r10
+	bgt	flush_levels
+finished:
+	mov	r10, #0				@ swith back to cache level 0
+	mcr	p15, 2, r10, c0, c0, 0		@ select current cache level in cssr
+	dsb	st
+	isb
+	bx	lr
+ENDPROC(__v7_flush_dcache_all)
+
+ENTRY(v7_flush_dcache_all)
+ ARM(	stmfd	sp!, {r4-r5, r7, r9-r11, lr}	)
+ THUMB(	stmfd	sp!, {r4-r7, r9-r11, lr}	)
+	bl	__v7_flush_dcache_all
+ ARM(	ldmfd	sp!, {r4-r5, r7, r9-r11, lr}	)
+ THUMB(	ldmfd	sp!, {r4-r7, r9-r11, lr}	)
+	bx	lr
+ENDPROC(v7_flush_dcache_all)
+
+/*
+ *	v7_invalidate_dcache_all()
+ *
+ *	Invalidate the whole D-cache.
+ *
+ *	Corrupted registers: r0-r7, r9-r11 (r6 only in Thumb mode)
+ *
+ *	Note: copied from __v7_flush_dcache_all above with
+ *	mcr     p15, 0, r11, c7, c14, 2
+ *	Replaced with:
+ *	mcr     p15, 0, r11, c7, c6, 2
+ */
+ENTRY(__v7_invalidate_dcache_all)
+	dmb					@ ensure ordering with previous memory accesses
+	mrc	p15, 1, r0, c0, c0, 1		@ read clidr
+	mov	r3, r0, lsr #23			@ move LoC into position
+	ands	r3, r3, #7 << 1			@ extract LoC*2 from clidr
+	beq	inval_finished			@ if loc is 0, then no need to clean
+	mov	r10, #0				@ start clean at cache level 0
+inval_levels:
+	add	r2, r10, r10, lsr #1		@ work out 3x current cache level
+	mov	r1, r0, lsr r2			@ extract cache type bits from clidr
+	and	r1, r1, #7			@ mask of the bits for current cache only
+	cmp	r1, #2				@ see what cache we have at this level
+	blt	inval_skip			@ skip if no cache, or just i-cache
+	mcr	p15, 2, r10, c0, c0, 0		@ select current cache level in cssr
+	isb					@ isb to sych the new cssr&csidr
+	mrc	p15, 1, r1, c0, c0, 0		@ read the new csidr
+	and	r2, r1, #7			@ extract the length of the cache lines
+	add	r2, r2, #4			@ add 4 (line length offset)
+	movw	r4, #0x3ff
+	ands	r4, r4, r1, lsr #3		@ find maximum number on the way size
+	clz	r5, r4				@ find bit position of way size increment
+	movw	r7, #0x7fff
+	ands	r7, r7, r1, lsr #13		@ extract max number of the index size
+inval_loop1:
+	mov	r9, r7				@ create working copy of max index
+inval_loop2:
+ ARM(	orr	r11, r10, r4, lsl r5	)	@ factor way and cache number into r11
+ THUMB(	lsl	r6, r4, r5		)
+ THUMB(	orr	r11, r10, r6		)	@ factor way and cache number into r11
+ ARM(	orr	r11, r11, r9, lsl r2	)	@ factor index number into r11
+ THUMB(	lsl	r6, r9, r2		)
+ THUMB(	orr	r11, r11, r6		)	@ factor index number into r11
+	mcr	p15, 0, r11, c7, c6, 2		@ invalidate by set/way
+	subs	r9, r9, #1			@ decrement the index
+	bge	inval_loop2
+	subs	r4, r4, #1			@ decrement the way
+	bge	inval_loop1
+inval_skip:
+	add	r10, r10, #2			@ increment cache number
+	cmp	r3, r10
+	bgt	inval_levels
+inval_finished:
+	mov	r10, #0				@ swith back to cache level 0
+	mcr	p15, 2, r10, c0, c0, 0		@ select current cache level in cssr
+	dsb	st
+	isb
+	bx	lr
+ENDPROC(__v7_invalidate_dcache_all)
+
+ENTRY(v7_invalidate_dcache_all)
+ ARM(	stmfd	sp!, {r4-r5, r7, r9-r11, lr}	)
+ THUMB(	stmfd	sp!, {r4-r7, r9-r11, lr}	)
+	bl	__v7_invalidate_dcache_all
+ ARM(	ldmfd	sp!, {r4-r5, r7, r9-r11, lr}	)
+ THUMB(	ldmfd	sp!, {r4-r7, r9-r11, lr}	)
+	bx	lr
+ENDPROC(v7_invalidate_dcache_all)
diff --git a/arch/arm/include/asm/armv7.h b/arch/arm/include/asm/armv7.h
index 30e7939..2fb824b 100644
--- a/arch/arm/include/asm/armv7.h
+++ b/arch/arm/include/asm/armv7.h
@@ -1,9 +1,8 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
 /*
  * (C) Copyright 2010
  * Texas Instruments, <www.ti.com>
  * Aneesh V <aneesh@ti.com>
- *
- * SPDX-License-Identifier:	GPL-2.0+
  */
 #ifndef ARMV7_H
 #define ARMV7_H
@@ -59,26 +58,28 @@
 #ifndef __ASSEMBLY__
 #include <linux/types.h>
 #include <asm/io.h>
+#include <asm/barriers.h>
 
-/*
- * CP15 Barrier instructions
- * Please note that we have separate barrier instructions in ARMv7
- * However, we use the CP15 based instructtions because we use
- * -march=armv5 in U-Boot
- */
-#define CP15ISB	asm volatile ("mcr     p15, 0, %0, c7, c5, 4" : : "r" (0))
-#define CP15DSB	asm volatile ("mcr     p15, 0, %0, c7, c10, 4" : : "r" (0))
-#define CP15DMB	asm volatile ("mcr     p15, 0, %0, c7, c10, 5" : : "r" (0))
-
-#ifdef __ARM_ARCH_7A__
-#define ISB	asm volatile ("isb" : : : "memory")
-#define DSB	asm volatile ("dsb" : : : "memory")
-#define DMB	asm volatile ("dmb" : : : "memory")
-#else
-#define ISB	CP15ISB
-#define DSB	CP15DSB
-#define DMB	CP15DMB
-#endif
+/* read L2 control register (L2CTLR) */
+static inline uint32_t read_l2ctlr(void)
+{
+	uint32_t val = 0;
+
+	asm volatile ("mrc p15, 1, %0, c9, c0, 2" : "=r" (val));
+
+	return val;
+}
+
+/* write L2 control register (L2CTLR) */
+static inline void write_l2ctlr(uint32_t val)
+{
+	/*
+	 * Note: L2CTLR can only be written when the L2 memory system
+	 * is idle, ie before the MMU is enabled.
+	 */
+	asm volatile("mcr p15, 1, %0, c9, c0, 2" : : "r" (val) : "memory");
+	isb();
+}
 
 /*
  * Workaround for ARM errata # 798870
@@ -145,6 +146,8 @@ void _smp_pen(void);
 
 extern char __secure_start[];
 extern char __secure_end[];
+extern char __secure_stack_start[];
+extern char __secure_stack_end[];
 
 #endif /* CONFIG_ARMV7_NONSEC */
 
diff --git a/arch/arm/include/asm/barriers.h b/arch/arm/include/asm/barriers.h
new file mode 100644
index 0000000..75b9eb4
--- /dev/null
+++ b/arch/arm/include/asm/barriers.h
@@ -0,0 +1,50 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (C) 2016 ARM Ltd.
+ *
+ * ARM and ARM64 barrier instructions
+ * split from armv7.h to allow sharing between ARM and ARM64
+ *
+ * Original copyright in armv7.h was:
+ * (C) Copyright 2010 Texas Instruments, <www.ti.com> Aneesh V <aneesh@ti.com>
+ *
+ * Much of the original barrier code was contributed by:
+ *   Valentine Barshak <valentine.barshak@cogentembedded.com>
+ */
+#ifndef __BARRIERS_H__
+#define __BARRIERS_H__
+
+#ifndef __ASSEMBLY__
+
+#ifndef CONFIG_ARM64
+/*
+ * CP15 Barrier instructions
+ * Please note that we have separate barrier instructions in ARMv7
+ * However, we use the CP15 based instructtions because we use
+ * -march=armv5 in U-Boot
+ */
+#define CP15ISB	asm volatile ("mcr     p15, 0, %0, c7, c5, 4" : : "r" (0))
+#define CP15DSB	asm volatile ("mcr     p15, 0, %0, c7, c10, 4" : : "r" (0))
+#define CP15DMB	asm volatile ("mcr     p15, 0, %0, c7, c10, 5" : : "r" (0))
+
+#endif /* !CONFIG_ARM64 */
+
+#if __LINUX_ARM_ARCH__ >= 7
+#define ISB	asm volatile ("isb sy" : : : "memory")
+#define DSB	asm volatile ("dsb sy" : : : "memory")
+#define DMB	asm volatile ("dmb sy" : : : "memory")
+#elif __LINUX_ARM_ARCH__ == 6
+#define ISB	CP15ISB
+#define DSB	CP15DSB
+#define DMB	CP15DMB
+#else
+#define ISB	asm volatile ("" : : : "memory")
+#define DSB	CP15DSB
+#define DMB	asm volatile ("" : : : "memory")
+#endif
+
+#define isb()	ISB
+#define dsb()	DSB
+#define dmb()	DMB
+#endif	/* __ASSEMBLY__ */
+#endif	/* __BARRIERS_H__ */
diff --git a/arch/arm/include/asm/cache.h b/arch/arm/include/asm/cache.h
index 1f63127..9747192 100644
--- a/arch/arm/include/asm/cache.h
+++ b/arch/arm/include/asm/cache.h
@@ -29,6 +29,8 @@ static inline void invalidate_l2_cache(void)
 }
 #endif
 
+int check_range(unsigned long start, unsigned long end);
+
 void l2_cache_enable(void);
 void l2_cache_disable(void);
 void set_section_dcache(int section, enum dcache_option option);
diff --git a/arch/arm/lib/cache.c b/arch/arm/lib/cache.c
index 3bd8710..902e406 100644
--- a/arch/arm/lib/cache.c
+++ b/arch/arm/lib/cache.c
@@ -10,6 +10,11 @@
 #include <common.h>
 #include <malloc.h>
 
+#ifndef CONFIG_SYS_CACHELINE_SIZE
+#define CONFIG_SYS_CACHELINE_SIZE 32
+#endif
+
+
 /*
  * Flush range from all levels of d-cache/unified-cache.
  * Affects the range [start, start + size - 1].
@@ -46,6 +51,26 @@ __weak void flush_dcache_range(unsigned long start, unsigned long stop)
 	/* An empty stub, real implementation should be in platform code */
 }
 
+
+int check_cache_range(unsigned long start, unsigned long stop)
+{
+	int ok = 1;
+
+	if (start & (CONFIG_SYS_CACHELINE_SIZE - 1))
+		ok = 0;
+
+	if (stop & (CONFIG_SYS_CACHELINE_SIZE - 1))
+		ok = 0;
+
+	if (!ok) {
+		debug("CACHE: Misaligned operation at range [%08lx, %08lx]\n",
+		      start, stop);
+	}
+
+	return ok;
+}
+
+
 #ifdef CONFIG_SYS_NONCACHED_MEMORY
 /*
  * Reserve one MMU section worth of address space below the malloc() area that
-- 
2.7.4

